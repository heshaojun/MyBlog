{
  "code": 200,
  "msg": "ok",
  "data": {
    "title": "文章标题",
    "summery": "文章概要",
    "type": "文章类型",
    "time": "2020-10-30 10:44:23",
    "visit": "10万+",
    "comment": "100",
    "id": "dfdsfjsdl",
    "context": "<!--- SPDX-License-Identifier: Apache-2.0 -->\n\n<p align=\"center\"><img width=\"40%\" src=\"docs/ONNX_logo_main.png\" /></p>\n\n[![Build Status](https://img.shields.io/azure-devops/build/onnx-pipelines/onnx/7?label=Linux&logo=Azure-Pipelines)](https://dev.azure.com/onnx-pipelines/onnx/_build/latest?definitionId=7&branchName=master)\n[![Build Status](https://img.shields.io/azure-devops/build/onnx-pipelines/onnx/5?label=Windows&logo=Azure-Pipelines)](https://dev.azure.com/onnx-pipelines/onnx/_build/latest?definitionId=5&branchName=master)\n[![Build Status](https://img.shields.io/azure-devops/build/onnx-pipelines/onnx/6?label=MacOS&logo=Azure-Pipelines)](https://dev.azure.com/onnx-pipelines/onnx/_build/latest?definitionId=6&branchName=master)\n[![CII Best Practices](https://bestpractices.coreinfrastructure.org/projects/3313/badge)](https://bestpractices.coreinfrastructure.org/projects/3313)\n\n[Open Neural Network Exchange (ONNX)](https://onnx.ai) is an open ecosystem that empowers AI developers\nto choose the right tools as their project evolves. ONNX provides an open source format for AI models, both deep learning and traditional ML. It defines an extensible computation graph model, as well as definitions of built-in operators and standard\ndata types. Currently we focus on the capabilities needed for inferencing (scoring).\n\nONNX is [widely supported](http://onnx.ai/supported-tools) and can be found in many frameworks, tools, and hardware. Enabling interoperability between different frameworks and streamlining the path from research to production helps increase the speed of innovation in the AI community. We invite the community to join us and further evolve ONNX.\n\n# Use ONNX\n* [Tutorials for creating ONNX models](https://github.com/onnx/tutorials).\n* [Pre-trained ONNX models](https://github.com/onnx/models)\n\n# Learn about the ONNX spec\n* [Overview](docs/Overview.md)\n* [ONNX intermediate representation spec](docs/IR.md)\n* [Versioning principles of the spec](docs/Versioning.md)\n* [Operators documentation](docs/Operators.md)\n* [Python API Overview](docs/PythonAPIOverview.md)\n\n# Programming utilities for working with ONNX Graphs\n* [Shape and Type Inference](docs/ShapeInference.md)\n* [Graph Optimization](docs/Optimizer.md)\n* [Opset Version Conversion](docs/VersionConverter.md)\n\n# Contribute\nONNX is a [community project](community). We encourage you to join the effort and contribute feedback, ideas, and code. You can participate in the [SIGs](community/sigs.md) and [Working Groups](community/working-groups.md) to shape the future of ONNX.\n\nCheck out our [contribution guide](https://github.com/onnx/onnx/blob/master/docs/CONTRIBUTING.md) to get started.\n\nIf you think some operator should be added to ONNX specification, please read\n[this document](docs/AddNewOp.md).\n\n# Discuss\nWe encourage you to open [Issues](https://github.com/onnx/onnx/issues), or use [Slack](https://slack.lfai.foundation/) for more real-time discussion\n\n# Follow Us\nStay up to date with the latest ONNX news. [[Facebook](https://www.facebook.com/onnxai/)] [[Twitter](https://twitter.com/onnxai)]\n\n\n\n\n\n\n# Installation\n\n## Binaries\n\nA binary build of ONNX is available from [Conda](https://conda.io), in [conda-forge](https://conda-forge.org/):\n\n```\nconda install -c conda-forge onnx\n```\n\n## Source\n\nIf you have installed onnx on your machine, please `pip uninstall onnx` first before the following process of build from source. \n\n### Linux and MacOS\nYou will need an install of Protobuf and NumPy to build ONNX.  One easy\nway to get these dependencies is via\n[Anaconda](https://www.anaconda.com/download/):\n\n```\n# Use conda-forge protobuf, as default doesn't come with protoc\nconda install -c conda-forge protobuf numpy\n```\n\nYou can then install ONNX from PyPi (Note: Set environment variable `ONNX_ML=1` for onnx-ml):\n\n```\npip install onnx\n```\n\nAlternatively, you can also build and install ONNX locally from source code:\n\n```\ngit clone https://github.com/onnx/onnx.git\ncd onnx\ngit submodule update --init --recursive\npython setup.py install\n```\n\nNote: When installing in a non-Anaconda environment, make sure to install the Protobuf compiler before running the pip installation of onnx. For example, on Ubuntu:\n\n```\nsudo apt-get install protobuf-compiler libprotoc-dev\npip install onnx\n```\n\n### Windows\nIf you are building ONNX from source on Windows, it is recommended that you also build Protobuf locally as a static library. The version distributed with conda-forge is a DLL and this is a conflict as ONNX expects it to be a static library.\n\nNote that the instructions in this README assume you are using Visual Studio. It is recommended that you run all the commands from a shell started from \"Developer Command Prompt for VS 2019\" and keep the build system generator for cmake (e.g., cmake -G \"Visual Studio 16 2019\") consistent.\n\n#### Build Protobuf and ONNX on Windows\nStep 1: Build Protobuf locally\n```\ngit clone https://github.com/protocolbuffers/protobuf.git\ncd protobuf\ngit checkout 3.11.x\ncd cmake\n# Explicitly set -Dprotobuf_MSVC_STATIC_RUNTIME=OFF to make sure protobuf does not statically link to runtime library\ncmake -G -A -Dprotobuf_MSVC_STATIC_RUNTIME=OFF -Dprotobuf_BUILD_TESTS=OFF -Dprotobuf_BUILD_EXAMPLES=OFF -DCMAKE_INSTALL_PREFIX=<protobuf_install_dir>\n# For example:\n# cmake -G \"Visual Studio 16 2019\" -A x64 -Dprotobuf_MSVC_STATIC_RUNTIME=OFF -Dprotobuf_BUILD_TESTS=OFF -Dprotobuf_BUILD_EXAMPLES=OFF -DCMAKE_INSTALL_PREFIX=..\\install\nmsbuild protobuf.sln /m /p:Configuration=Release\nmsbuild INSTALL.vcxproj /p:Configuration=Release\n```\n\nStep 2: Build ONNX\n```\n# Get ONNX\ngit clone https://github.com/onnx/onnx.git\ncd onnx\ngit submodule update --init --recursive\n\n# Set environment variables to find protobuf and turn off static linking of ONNX to runtime library.\n# Even better option is to add it to user\\system PATH so this step can be performed only once.\n# For more details check https://docs.microsoft.com/en-us/cpp/build/reference/md-mt-ld-use-run-time-library?view=vs-2017\nset PATH=<protobuf_install_dir>\\bin;<protobuf_install_dir>\\include;<protobuf_install_dir>\\libs;%PATH%\nset USE_MSVC_STATIC_RUNTIME=0\n\n# use the static installed protobuf\nset CMAKE_ARGS=-DONNX_USE_PROTOBUF_SHARED_LIBS=OFF -DProtobuf_USE_STATIC_LIBS=ON\n\n# Optional: Set environment variable `ONNX_ML=1` for onnx-ml\n\n# Build ONNX\npython setup.py install\n```\n\nIf you would prefer to use Protobuf from conda-forge instead of building Protobuf from source, you can use the following instructions.\n\n#### Build ONNX on Windows with Anaconda\n\n```\n# Use conda-forge protobuf\nconda install -c conda-forge numpy libprotobuf=3.11.3 protobuf\n\n# Get ONNX\ngit clone https://github.com/onnx/onnx.git\ncd onnx\ngit submodule update --init --recursive\n\n# Set environment variable for ONNX to use protobuf shared lib\nset USE_MSVC_STATIC_RUNTIME=0\nset CMAKE_ARGS=-DONNX_USE_PROTOBUF_SHARED_LIBS=ON -DProtobuf_USE_STATIC_LIBS=OFF -DONNX_USE_LITE_PROTO=ON\n\n# Build ONNX\n# Optional: Set environment variable `ONNX_ML=1` for onnx-ml\n\npython setup.py install\n```\n\n#### Build ONNX on ARM 64\nIf you are building ONNX on an ARM 64 device, please make sure to install the dependencies appropriately.\n\n```\npip install cython protobuf numpy\nsudo apt-get install libprotobuf-dev protobuf-compiler\npip install onnx\n```\n\n## Verify Installation\nAfter installation, run\n\n```\npython -c \"import onnx\"\n```\n\nto verify it works.\n\n\n#### Common Errors\n**Environment variables**: `USE_MSVC_STATIC_RUNTIME` (should be 1 or 0, not ON or OFF)\n\n**CMake variables**: `ONNX_USE_PROTOBUF_SHARED_LIBS`, `Protobuf_USE_STATIC_LIBS`\n\nIf `ONNX_USE_PROTOBUF_SHARED_LIBS` is ON then `Protobuf_USE_STATIC_LIBS` must be OFF and `USE_MSVC_STATIC_RUNTIME` must be 0.\nIf `ONNX_USE_PROTOBUF_SHARED_LIBS` is OFF then `Protobuf_USE_STATIC_LIBS` must be ON and `USE_MSVC_STATIC_RUNTIME` can be 1 or 0.\n\nNote that the `import onnx` command does not work from the source checkout directory; in this case you'll see `ModuleNotFoundError: No module named 'onnx.onnx_cpp2py_export'`. Change into another directory to fix this error.\n\nBuilding ONNX on Ubuntu works well, but on CentOS/RHEL and other ManyLinux systems, you might need to open the [CMakeLists file](https://github.com/onnx/onnx/blob/master/CMakeLists.txt#L124) and replace all instances of `/lib` with `/lib64`.\n\nIf you want to build ONNX on Debug mode, remember to set the environment variable `DEBUG=1`. For debug versions of the dependencies, you need to open the [CMakeLists file](CMakeLists.txt) and append a letter `d` at the end of the package name lines. For example, `NAMES protobuf-lite` would become `NAMES protobuf-lited`.\n\nYou can also use the [onnx-dev docker image](https://hub.docker.com/r/onnx/onnx-dev) for a Linux-based installation without having to worry about dependency versioning.\n\n# Testing\n\nONNX uses [pytest](https://docs.pytest.org) as test driver. In order to run tests, you will first need to install pytest:\n\n```\npip install pytest nbval\n```\n\nAfter installing pytest, use the following command to run tests.\n\n```\npytest\n```\n\n# Development\n\nCheck out the [contributor guide](https://github.com/onnx/onnx/blob/master/docs/CONTRIBUTING.md) for instructions.\n\n# License\n\n[Apache License v2.0](LICENSE)\n\n# Code of Conduct\n\n[ONNX Open Source Code of Conduct](https://onnx.ai/codeofconduct.html)",
    "pre": {
      "title": "dsfdsf",
      "id": "sdfdsfads"
    },
    "next": null
  }
}